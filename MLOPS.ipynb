{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCZvTpNLo0Td"
      },
      "outputs": [],
      "source": [
        "#Cloudera \n",
        "\n",
        "MLOPS pipeline with spark streaming and Kafka on cloudera\n",
        "\n",
        "from pyspark.sql.functions import from_json, col, explode\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "import mlflow\n",
        "from datetime import datetime, timedelta\n",
        "Define constants\n",
        "SPARK_APP_NAME = \"MLOps-Spark-Streaming-Kafka\"\n",
        "SPARK_MASTER = \"local[*]\"\n",
        "SPARK_BATCH_INTERVAL = 1800  # 30 minutes\n",
        "MODEL_PATH = \"/path/to/model\"\n",
        "FEATURE_COLUMNS = [\"feature1\", \"feature2\", \"feature3\", \"feature4\"]\n",
        "KAFKA_BOOTSTRAP_SERVERS = \"kafka-broker1:9092,kafka-broker2:9092\"\n",
        "KAFKA_TOPIC = \"my-topic\"\n",
        "KAFKA_STARTING_OFFSETS = \"earliest\"\n",
        "KAFKA_CHECKPOINT_LOCATION = \"/path/to/checkpoint\"\n",
        "Define schema for incoming Kafka messages\n",
        "kafka_schema = StructType([\n",
        "    StructField(\"label\", DoubleType()),\n",
        "    StructField(\"feature1\", DoubleType()),\n",
        "    StructField(\"feature2\", DoubleType()),\n",
        "    StructField(\"feature3\", DoubleType()),\n",
        "    StructField(\"feature4\", DoubleType())\n",
        "])\n",
        "Create Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(SPARK_APP_NAME) \\\n",
        "    .master(SPARK_MASTER) \\\n",
        "    .getOrCreate()\n",
        "Create MLflow experiment\n",
        "mlflow.set_experiment(\"MLOps-Spark-Streaming-Kafka\")\n",
        "Define Kafka data stream source and preprocessing\n",
        "raw_stream = spark.readStream.format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
        "    .option(\"subscribe\", KAFKA_TOPIC) \\\n",
        "    .option(\"startingOffsets\", KAFKA_STARTING_OFFSETS) \\\n",
        "    .load()\n",
        "preprocessed_stream = raw_stream.selectExpr(\"CAST(value AS STRING)\") \\\n",
        "    .select(from_json(col(\"value\"), kafka_schema).alias(\"json\")) \\\n",
        "    .select(\"json.*\")\n",
        "Define model training function\n",
        "def train_model():\n",
        "    with mlflow.start_run():\n",
        "        # Define model and training parameters\n",
        "        lr = LogisticRegression()\n",
        "        assembler = VectorAssembler(inputCols=FEATURE_COLUMNS, outputCol=\"features\")\n",
        "        evaluator = BinaryClassificationEvaluator()\n",
        "        # Split preprocessed data into training and testing sets\n",
        "        train_data, test_data = preprocessed_stream.randomSplit([0.7, 0.3], seed=123)\n",
        "        # Assemble feature vector\n",
        "        train_data = assembler.transform(train_data).select(\"features\", \"label\")\n",
        "        test_data = assembler.transform(test_data).select(\"features\", \"label\")\n",
        "        # Train model\n",
        "        model = lr.fit(train_data)\n",
        "        # Evaluate model\n",
        "        auc = evaluator.evaluate(model.transform(test_data))\n",
        "        mlflow.log_metric(\"auc\", auc)\n",
        "        # Save model\n",
        "        model.write().overwrite().save(MODEL_PATH)\n",
        "        mlflow.log_artifact(MODEL_PATH, \"model\")\n",
        "Define streaming query and start it\n",
        "training_query = preprocessed_stream.writeStream \\\n",
        "    .trigger(processingTime=f\"{SPARK_BATCH_INTERVAL} seconds\") \\\n",
        "    .option(\"checkpointLocation\", KAFKA_CHECKPOINT_LOCATION) \\\n",
        "    .foreachBatch(lambda batch_df, batch_id: train_model()) \\\n",
        "    .start()\n",
        "Wait for the query to terminate\n",
        "training_query.awaitTermination()\n",
        "\n",
        "we read streaming data from Kafka and preprocess it using a specified schema. We then define a training function that trains\n",
        "\n",
        "Code for an MLOps pipeline which reads a Kafka topic into Spark structured streaming, performs classification with a classification model, and then writes the output to a new Kafka topic every 30 minutes\n",
        "\n",
        "from pyspark.sql.functions import from_json, col, to_json, struct\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegressionModel\n",
        "from pyspark.ml import PipelineModel\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.streaming import StreamingContext\n",
        "from kafka import KafkaProducer\n",
        "from datetime import datetime, timedelta\n",
        "Define constants\n",
        "APP_NAME = \"MLOps-Kafka-Spark-Streaming\"\n",
        "KAFKA_BOOTSTRAP_SERVERS = \"localhost:9092\"\n",
        "INPUT_TOPIC = \"input-topic\"\n",
        "OUTPUT_TOPIC = \"output-topic\"\n",
        "BATCH_DURATION = 1800  # 30 minutes\n",
        "MODEL_PATH = \"/path/to/model\"\n",
        "FEATURE_COLUMNS = [\"feature1\", \"feature2\", \"feature3\", \"feature4\"]\n",
        "Create Spark session and streaming context\n",
        "spark = SparkSession.builder.appName(APP_NAME).getOrCreate()\n",
        "ssc = StreamingContext(spark.sparkContext, BATCH_DURATION)\n",
        "Define input stream from Kafka topic\n",
        "kafka_stream = spark \\\n",
        "    .readStream \\\n",
        "    .format(\"kafka\") \\\n",
        "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
        "    .option(\"subscribe\", INPUT_TOPIC) \\\n",
        "    .load()\n",
        "Parse input stream data from JSON to DataFrame\n",
        "input_schema = StructType([\n",
        "    StructField(\"feature1\", DoubleType()),\n",
        "    StructField(\"feature2\", DoubleType()),\n",
        "    StructField(\"feature3\", DoubleType()),\n",
        "    StructField(\"feature4\", DoubleType())\n",
        "])\n",
        "parsed_stream = kafka_stream \\\n",
        "    .selectExpr(\"CAST(value AS STRING)\") \\\n",
        "    .select(from_json(col(\"value\"), input_schema).alias(\"parsed_data\")) \\\n",
        "    .select(\"parsed_data.*\")\n",
        "Assemble features vector\n",
        "assembler = VectorAssembler(inputCols=FEATURE_COLUMNS, outputCol=\"features\")\n",
        "assembled_stream = assembler.transform(parsed_stream).select(\"features\")\n",
        "Load model\n",
        "model = PipelineModel.load(MODEL_PATH)\n",
        "Predict on streaming data and write output to new Kafka topic\n",
        "def classify_and_write_output(batch_df, batch_id):\n",
        "    predictions = model.transform(batch_df)\n",
        "    output_df = predictions.select(to_json(struct(col(\"*\"))).alias(\"value\"))\n",
        "    output_df \\\n",
        "        .write \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
        "        .option(\"topic\", OUTPUT_TOPIC) \\\n",
        "        .save()\n",
        "Create streaming job\n",
        "query = assembled_stream \\\n",
        "    .writeStream \\\n",
        "    .foreachBatch(classify_and_write_output) \\\n",
        "    .trigger(processingTime=str(BATCH_DURATION) + \" seconds\") \\\n",
        "    .start()\n",
        "Wait for the streaming job to finish\n",
        "query.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "structured streaming pipeline MLOPs pipeline for classification machine learning model with trigger of new data ingestion and write output to new kafka topic\n",
        "\n",
        "Code Snippet - \n",
        "\n",
        "import necessary libraries\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "from pyspark.sql.functions import from_json, col, struct, to_json\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType\n",
        "from pyspark.sql import SparkSession\n",
        "create SparkSession\n",
        "spark = SparkSession.builder.appName(\"MLOpsPipeline\").getOrCreate()\n",
        "define schema for incoming data\n",
        "input_schema = StructType([\n",
        "  StructField(\"feature1\", DoubleType(), True),\n",
        "  StructField(\"feature2\", DoubleType(), True),\n",
        "  StructField(\"feature3\", DoubleType(), True),\n",
        "  StructField(\"label\", DoubleType(), True)\n",
        "])\n",
        "define schema for output data\n",
        "output_schema = StructType([\n",
        "  StructField(\"prediction\", DoubleType(), True),\n",
        "  StructField(\"probability\", StringType(), True)\n",
        "])\n",
        "read incoming data from Kafka\n",
        "df = spark \\\n",
        "  .readStream \\\n",
        "  .format(\"kafka\") \\\n",
        "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "  .option(\"subscribe\", \"input_topic\") \\\n",
        "  .option(\"startingOffsets\", \"earliest\") \\\n",
        "  .load()\n",
        "convert the incoming data to a dataframe with the defined schema\n",
        "df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
        "  .select(from_json(col(\"value\"), input_schema).alias(\"data\")) \\\n",
        "  .select(col(\"data.*\"))\n",
        "create feature vector assembler\n",
        "assembler = VectorAssembler(\n",
        "  inputCols=[\"feature1\", \"feature2\", \"feature3\"],\n",
        "  outputCol=\"features\"\n",
        ")\n",
        "create logistic regression model\n",
        "lr = LogisticRegression(\n",
        "  featuresCol=\"features\",\n",
        "  labelCol=\"label\",\n",
        "  predictionCol=\"prediction\",\n",
        "  probabilityCol=\"probability\",\n",
        "  maxIter=10\n",
        ")\n",
        "create a pipeline for feature vector assembler and logistic regression model\n",
        "pipeline = Pipeline(stages=[assembler, lr])\n",
        "fit the pipeline to the training data\n",
        "model = pipeline.fit(df)\n",
        "write output to Kafka\n",
        "output = model.transform(df) \\\n",
        "  .select(to_json(struct(col(\"*\"))).alias(\"value\")) \\\n",
        "  .writeStream \\\n",
        "  .format(\"kafka\") \\\n",
        "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
        "  .option(\"checkpointLocation\", \"checkpoints\") \\\n",
        "  .option(\"topic\", \"output_topic\") \\\n",
        "  .trigger(processingTime='10 seconds') \\\n",
        "  .start()\n",
        "start the stream\n",
        "output.awaitTermination()\n",
        "\n",
        "Explaination - \n",
        "\n",
        "Import necessary libraries - importing required packages and libraries for building the pipeline.\n",
        "Create SparkSession - create a new Spark session with the name \"MLOpsPipeline\".\n",
        "Define schema for incoming data - define the schema of the incoming data, including feature1, feature2, feature3, and label.\n",
        "Define schema for output data - define the schema of the output data, including prediction and probability.\n",
        "Read incoming data from Kafka - read incoming data from Kafka with the defined schema.\n",
        "Convert the incoming data to a dataframe with the defined schema - convert the incoming data to a dataframe with the defined schema by using the from_json() function.\n",
        "Create feature vector assembler - create a feature vector assembler to assemble feature1, feature2, and feature3 into a single feature column called \"features\".\n",
        "Create logistic regression model - create a logistic regression model with featuresCol=\"features\", labelCol=\"label\", predictionCol=\"prediction\", probabilityCol=\"probability\", and maxIter=10.\n",
        "Create a pipeline for feature vector assembler and logistic regression model - create a pipeline with the assembler and logistic regression model.\n",
        "Fit the pipeline to the training data - fit the pipeline to the training data.\n",
        "Write output to Kafka - write the output to Kafka with to_json() function, where we select all columns and convert them to a JSON string.\n",
        "Start the stream - start the stream with awaitTermination() function.\n",
        "\n",
        "Mlops pipeline with spark stuctured streaming which trigger on everytime new data ingested after 24 hours  and train the model and save to model to desktop directory submit and  this as spark job \n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "import time\n",
        "Define the data schema\n",
        "data_schema = \"feature1 FLOAT, feature2 FLOAT, feature3 FLOAT, feature4 FLOAT, label INT\"\n",
        "Define the batch interval (in seconds) for streaming\n",
        "batch_interval = 60\n",
        "Define the directory to save the trained model\n",
        "model_directory = \"/Users/myusername/Desktop/my_model\"\n",
        "Create a SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MLOps-Spark-Streaming\") \\\n",
        "    .getOrCreate()\n",
        "Define the streaming data source\n",
        "streaming_df = spark.readStream \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .schema(data_schema) \\\n",
        "    .load(\"/path/to/streaming/data/directory\")\n",
        "Define the preprocessing steps\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"feature1\", \"feature2\", \"feature3\", \"feature4\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "Define the training function\n",
        "def train_model(df):\n",
        "    if df.count() > 0:\n",
        "        # Preprocess the data\n",
        "        preprocessed_df = assembler.transform(df)\n",
        "        # Train the model\n",
        "        model = lr.fit(preprocessed_df)\n",
        "        # Save the model\n",
        "        model.save(model_directory)\n",
        "\n",
        "Define the training step\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "Define the streaming query\n",
        "query = streaming_df \\\n",
        "    .withColumnRenamed(\"label\", \"target\") \\\n",
        "    .withWatermark(\"timestamp\", \"24 hours\") \\\n",
        "    .groupBy(window(\"timestamp\", \"24 hours\")) \\\n",
        "    .agg(avg(col(\"feature1\")).alias(\"feature1\"),\n",
        "         avg(col(\"feature2\")).alias(\"feature2\"),\n",
        "         avg(col(\"feature3\")).alias(\"feature3\"),\n",
        "         avg(col(\"feature4\")).alias(\"feature4\"),\n",
        "         max(col(\"target\")).alias(\"label\")) \\\n",
        "    .withColumn(\"timestamp\", current_timestamp()) \\\n",
        "    .select(\"timestamp\", \"feature1\", \"feature2\", \"feature3\", \"feature4\", \"label\") \\\n",
        "    .writeStream \\\n",
        "    .trigger(processingTime=\"24 hours\") \\\n",
        "    .foreachBatch(lambda df, epoch_id: train_model(df)) \\\n",
        "    .start()\n",
        "\n",
        "Wait for the query to finish\n",
        "query.awaitTermination()\n",
        "Stop the SparkSession\n",
        "spark.stop()\n",
        "\n",
        "To submit this code as a Spark job, you can package the code into a JAR file using sbt or maven, and then submit the JAR file to a Spark cluster using the spark-submit command. Here's an example spark-submit command:\n",
        "\n",
        "$SPARK_HOME/bin/spark-submit \\\n",
        "    --class com.example.mlops.MLOpsPipeline \\\n",
        "    --master yarn \\\n",
        "    --deploy-mode client \\\n",
        "    my-mlops-pipeline.jar\n",
        "\n",
        "\n",
        "\n",
        "Mlops pipeline with MLFLOW which trigger on everytime new data ingested after 30 Minutes   and train the model and save to model to desktop directory submit and  this as spark job\n",
        "\n",
        "Import required libraries\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "import mlflow\n",
        "import os\n",
        "import time\n",
        "Define constants\n",
        "MODEL_DIR = \"file:///home/user/Desktop/model\"\n",
        "EXPERIMENT_NAME = \"My Experiment\"\n",
        "FEATURE_COLUMNS = [\"feature1\", \"feature2\", \"feature3\", \"feature4\"]\n",
        "TRIGGER_INTERVAL = 1800  # 30 minutes\n",
        "Start MLflow run\n",
        "with mlflow.start_run(run_name=\"My Run\") as run:\n",
        "    # Log experiment name and run ID\n",
        "    experiment_id = run.info.experiment_id\n",
        "    mlflow.log_param(\"experiment_id\", experiment_id)\n",
        "    mlflow.log_param(\"run_id\", run.info.run_id)\n",
        "    # Load data and split into training and validation sets\n",
        "    data = spark.read.csv(\"data.csv\", header=True, inferSchema=True)\n",
        "    training_data, validation_data = data.randomSplit([0.7, 0.3], seed=42)\n",
        "    # Preprocess data\n",
        "    assembler = VectorAssembler(inputCols=FEATURE_COLUMNS, outputCol=\"features\")\n",
        "    training_data = assembler.transform(training_data).select(\"features\", \"label\")\n",
        "    validation_data = assembler.transform(validation_data).select(\"features\", \"label\")\n",
        "    # Train model and log metrics\n",
        "    lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "    model = lr.fit(training_data)\n",
        "    train_predictions = model.transform(training_data)\n",
        "    train_accuracy = train_predictions.filter(train_predictions.label == train_predictions.prediction).count() / float(training_data.count())\n",
        "    mlflow.log_metric(\"train_accuracy\", train_accuracy)\n",
        "    validation_predictions = model.transform(validation_data)\n",
        "    validation_accuracy = validation_predictions.filter(validation_predictions.label == validation_predictions.prediction).count() / float(validation_data.count())\n",
        "    mlflow.log_metric(\"validation_accuracy\", validation_accuracy)\n",
        "    # Save model to desktop directory\n",
        "    model.save(MODEL_DIR)\n",
        "    # Log model artifacts\n",
        "    mlflow.log_artifact(MODEL_DIR)\n",
        "    # Sleep until next trigger interval\n",
        "    time.sleep(TRIGGER_INTERVAL)\n",
        "End MLflow run\n",
        "mlflow.end_run()\n",
        "\n",
        "To submit this code as a Spark job, you can package it as a JAR file and submit it using the spark-submit command. For example:\n",
        "\n",
        "$ sbt package\n",
        "$ spark-submit --class com.example.MyApp --master local[4] target/scala-2.12/myapp_2.12-1.0.jar\n",
        "\n",
        "Replace com.example.MyApp with the fully qualified name of your application class, and target/scala-2.12/myapp_2.12-1.0.jar with the path to your JAR file. You may also need to adjust the --master option depending on your Spark cluster setup."
      ],
      "metadata": {
        "id": "xbd-XjckpSmx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}